Title: KD-DLGAN: Data Limited Image Generation via Knowledge Distillation

Url: https://openaccess.thecvf.com/content/CVPR2023/papers/Cui_KD-DLGAN_Data_Limited_Image_Generation_via_Knowledge_Distillation_CVPR_2023_paper.pdf

We aim to reproduce the scores on table-4 as quantitative results and figures4 as the qualitative result.

—— version 1 submission ——
- We didn't change our goals, despite being able fully achieve them.

- We couldn't achieve the goals yet. In the DA paper, the recommended number of epochs was 3000. 
    However, due to time concerns in training KD-DLGAN, we could only train for 160 epochs. 
    Therefore, neither model matched our goals in terms of FID scores.
    
- Our initial plan is to train the models with more epochs as recommended. 
    If our results still do not match those reported in the paper, we may need to 
    review and potentially modify our implementation of KD-DLGAN distillation method.

—— version 2 submission ——
- Due to high time and computation requirements of each experiment, we decided to reproduce results 
    for only 100-shot Obama dataset, instead of all datasets in table-4.

- We could achieve to reproduce the results partially. Although, we cannot obtain the same results in 
    table-4 for DA (baseline configuration) and KD-DLGAN implementation, it is still enough to observe 
    the effect of proposed method. On the other hand we could not obtain the results for the configurations 
    where only AGKD enabled and only CGKD enabled.

- The main reason we had to change our goals, and could not reproduce them fully is the computational
    complexity of not only style-gan training process but also our own KD-DLGAN implementation. In each
    epochs, 75% percent of the time is spent due to our loss function implementation. If we were able to
    implement it more efficiently, we may also be able to do more extensive experiments.